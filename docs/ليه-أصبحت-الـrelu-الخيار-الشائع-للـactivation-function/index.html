<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ - Something I learned</title><meta name="Description" content=""><meta property="og:title" content="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ" />
<meta property="og:description" content="
    
        ReLU هو اختصار Rectified Linear Unit وهي واحدة من أكثر الـactivation functions المستخدمة في الـNeural networks، رغم إن اسمها يحتوى على كلمة linear لكنها non linear function، وده واضح رياضياً وحتى من الرسم بتاعها، لكن ليه بتقدر تنافس الـactivation functions الأخرى المكوّنة من smooth curves رغم إنها حرفياً عبارة عن خطين بس؟
    
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" /><meta property="og:image" content="https://Ahmad-Zaki.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-13T11:11:45+02:00" />
<meta property="article:modified_time" content="2021-11-13T11:11:45+02:00" /><meta property="og:site_name" content="Something I learned" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://Ahmad-Zaki.github.io/logo.png"/>

<meta name="twitter:title" content="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ"/>
<meta name="twitter:description" content="
    
        ReLU هو اختصار Rectified Linear Unit وهي واحدة من أكثر الـactivation functions المستخدمة في الـNeural networks، رغم إن اسمها يحتوى على كلمة linear لكنها non linear function، وده واضح رياضياً وحتى من الرسم بتاعها، لكن ليه بتقدر تنافس الـactivation functions الأخرى المكوّنة من smooth curves رغم إنها حرفياً عبارة عن خطين بس؟
    
"/>
<meta name="application-name" content="مما تعلمت">
<meta name="apple-mobile-web-app-title" content="مما تعلمت"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" /><link rel="prev" href="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/" /><link rel="next" href="https://Ahmad-Zaki.github.io/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/Ahmad-Zaki.github.io\/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function\/"
        },"genre": "posts","keywords": "Neural Networks, Deep Learning","wordcount":  323 ,
        "url": "https:\/\/Ahmad-Zaki.github.io\/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function\/","datePublished": "2021-11-13T11:11:45+02:00","dateModified": "2021-11-13T11:11:45+02:00","publisher": {
            "@type": "Organization",
            "name": "Ahmad Zaki"},"author": {
                "@type": "Person",
                "name": "Ahmad Zaki"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Something I learned"><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/" title="Know more about me!"> About </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Look for an article" id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Something I learned"><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Look for an article" id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="Know more about me!">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><article class="page single"><h1 class="single-title animated flipInX">؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ</h1><h2 class="single-subtitle">؟Activationsوايه يميزها عن باقي الـ</h2><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Ahmad Zaki</a></span>&nbsp;<span class="post-category">included in <a href="/categories/%D9%85%D9%85%D8%A7-%D8%AA%D8%B9%D9%84%D9%85%D8%AA/"><i class="far fa-folder fa-fw"></i>مما تعلمت</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-11-13">2021-11-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;323 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;2 minutes&nbsp;</div>
        </div><div class="content" id="content"><div dir="rtl">
    <h4>
        ReLU هو اختصار Rectified Linear Unit وهي واحدة من أكثر الـactivation functions المستخدمة في الـNeural networks، رغم إن اسمها يحتوى على كلمة linear لكنها non linear function، وده واضح رياضياً وحتى من الرسم بتاعها، لكن ليه بتقدر تنافس الـactivation functions الأخرى المكوّنة من smooth curves رغم إنها حرفياً عبارة عن خطين بس؟
    </h4>
</div>
<!--
helpful html tags:
<div dir="rtl"></div>

<br>

<hr style="height:0px;border:none;color:#333;border-top: 1px solid black">

-->
<p>$$
ReLU=max(0,x)
$$</p>
<p align="center">
    <img src="/lib/img/ReLU.JPG" alt="ReLU function Graph"/>
    <br>
    Fig 1: ReLU Graph
</p>
<div dir="rtl">
    ميزة الـReLU الكبرى مش بتظهر لما أستخدم واحدة بس منها، لكن بتكون واضحة لما أجمع شوية دوال ReLU كتير مع بعض، نقطة الانكسار الموجودة بين جزئين دالة الـReLU بتقدر تقرّبلك مجموعة الخطوط الكتيرة دى من أي منحنى أنت عايزه عن طريق إنها بتتيح لك تكسر الخط المستقيم في أي مكان انت عايزه (أو المكان الى الـneural network بتتعلمه).  
    <br><br>
    لاحظ الصورة التالية: لما استخدمت ReLU واحدة بس مقدمتش اختلاف كبير عن الـstraight line، لكن لما بدأت اعمل nesting واجمعهم مع بعض بدأت أكوّن شكل جديد مقدرتش أوصله لما جمعت الـstraight lines مع بعض.
    <p align="center">
        <img src="/lib/img/ReLU_vs_linear.jpg" alt="ReLU vs. linear function"/>
        <br>
        Fig 2: ReLU vs. Linear function
    </p>
    <br>
    <h3>
        طيب هل هي فعلاً تقدر توصل لمنحنيات صعبة زى باقي الـactivation function؟
    </h3>
    الإجابة القصيرة: نعم، وده واضح من خلال استخدامها الشائع والواسع في كل مجالات الـdeep learning، لاحظ الصورة التالية: عن طريق استخدام أكتر من ReLU function قدرت أوصل لشكل تقريبي من منحنى <bdo dir="ltr">$x^3+x^2-x-1$</bdo>
    <p align="center">
        <img src="/lib/img/ReLU_Approximation.JPG" alt="ReLU_Approximation"/>
        <br>
        Fig 3: We can use many ReLUs to approximate complex functions <br>
        Source: <a href="https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6">Click here</a>
    </p>
    <br>
    وبالنسبة للـNeural networks، في الصورة التالية نقدر نلاحظ إن neural network بتستخدم ReLU كـActivation function وصلت لمنحنى قريب من المنحنى التانج عن neural network بتستخد $tanh$ في نفس عدد الـepoch
    <p align="center">
        <img src="/lib/img/ReLU_vs_TANH_in_neural_networks.jpg" alt="ReLU vs tanh"/>
        <br>
        <p dir="rtl" align="center">Fig 4: كل network تحتوي 3 hidden layers، كل منهم يحتوى 3 neurons</b><br>
        Source: <a href="https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792">Click here</a>
    </p>
    <h3>
        طيب ما الـtanh وصلت لـsmooth curve وعملت نتيجة كويسة بردو، ليه اروح للـReLU؟
    </h3>  
    هنا تيجي ميزة الـReLU التانية والى بتديلها أفضلية كبيرة عن الـtanh: إنها very computationally efficient، هي في الآخر عبارة عن خط وتفاضلها ثابت، على عكس الـtanh الى فيها exponentials وتفاضلها متغير.  
</div>
<p><br><br></p>
<hr style="height:0px;border:none;color:#333;border-top: 1px solid black">
<div dir="rtl">
        <b>
        لمزيد من المعلومات
        </b>
</div>
<ul>
<li><a href="https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792" target="_blank" rel="noopener noreffer">Finally, an intuitive explanation of why ReLU works</a></li>
<li><a href="https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6" target="_blank" rel="noopener noreffer">Can neural networks solve any problem?</a></li>
</ul>
<br>
<hr style="height:0px;border:none;color:#333;border-top: 1px solid black"></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2021-11-13</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-title="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ" data-hashtags="Neural Networks,Deep Learning"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-hashtag="Neural Networks"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/"><i class="fab fa-linkedin fa-fw"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-title="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-title="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="Share on Myspace" data-sharer="myspace" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-title="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-title="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/" data-title="؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/neural-networks/">Neural Networks</a>,&nbsp;<a href="/tags/deep-learning/">Deep Learning</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/" class="prev" rel="prev" title="؟Neural networksفي الـ Non-linear activation functions ليه بنحتاج"><i class="fas fa-angle-left fa-fw"></i>؟Neural networksفي الـ Non-linear activation functions ليه بنحتاج</a>
            <a href="/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/" class="next" rel="next" title="؟Imbalanced Datasets كيف نتعامل مع">؟Imbalanced Datasets كيف نتعامل مع<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.90.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Ahmad Zaki</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"id-1":"Something I Learned","id-2":"Something I Learned"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
