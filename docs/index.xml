<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Something I learned</title>
        <link>https://Ahmad-Zaki.github.io/</link>
        <description>Something I learned</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>ahmadheshamzaki@gmail.com (Ahmad Zaki)</managingEditor>
            <webMaster>ahmadheshamzaki@gmail.com (Ahmad Zaki)</webMaster><lastBuildDate>Mon, 28 Feb 2022 14:20:12 &#43;0200</lastBuildDate>
            <atom:link href="https://Ahmad-Zaki.github.io/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>؟ConvNetsفي الـ Backpropagationكيف يتم تنفيذ الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%8A%D8%AA%D9%85-%D8%A7%D9%84%D9%80backpropagation-%D9%81%D9%8A-%D8%A7%D9%84%D9%80covolutional-neural-networks/</link>
    <pubDate>Mon, 28 Feb 2022 14:20:12 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%8A%D8%AA%D9%85-%D8%A7%D9%84%D9%80backpropagation-%D9%81%D9%8A-%D8%A7%D9%84%D9%80covolutional-neural-networks/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        في الغالب كانت بداية معرفتك بالـbackbropagation عن طريق شرحه على Fully connected neural network, لكن بعد ما تتطرق لأشكال أخرى من الـneural netowrks مثل الـconvolutional neural network بتلاقي إن الـbackpropagation مستخدم فيهم كـalgorithm رئيسي لحساب الـgradients في كل layer لكن بدون ذكر تفاصيل أكثر عنها، لإنك بالفعل غير مضطر لمعرفة تفاصيل الرياضية قبل استخدامها، ومع ذلك فمعرفة كيف تتم داخل الـConv layers  قد يكون سؤال مر ببالك ولو من باب الفضول في وقت من الأوقات.
    </h4>
</div>]]></description>
</item><item>
    <title>؟Vanishing gradientsكيف نتعامل مع مشكلة الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9-%D9%85%D8%B4%D9%83%D9%84%D8%A9-%D8%A7%D9%84%D9%80vanishing-gradients/</link>
    <pubDate>Tue, 14 Dec 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9-%D9%85%D8%B4%D9%83%D9%84%D8%A9-%D8%A7%D9%84%D9%80vanishing-gradients/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـVanishing Gradients هي أحد اكبر المشاكل الى أثرت على تطوّر الـneural networks لفترة طويلة، وبسببها كان تدريب الـdeep neural networks عملية صعبة وبتستغرق وقت كبير، لكن إيه السبب وراء المشكلة دى؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟K-meansمشاكل الـ Gaussian Mixture Modelsكيف تحل الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D8%AA%D8%AD%D9%84-%D8%A7%D9%84%D9%80gaussian-mixture-models-%D9%85%D8%B4%D8%A7%D9%83%D9%84-%D8%A7%D9%84%D9%80k-means/</link>
    <pubDate>Tue, 07 Dec 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D8%AA%D8%AD%D9%84-%D8%A7%D9%84%D9%80gaussian-mixture-models-%D9%85%D8%B4%D8%A7%D9%83%D9%84-%D8%A7%D9%84%D9%80k-means/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـGaussian Mixture models (GMM) هي عبارة عن probabilistic model بيقدر يعبر عن الـdata distribution وكأنه عبارة عن مجموعة gaussian models متجمعة مع بعض، وممكن نستخدمها كـclustering algorithm مشابه للـk-means . بس إزاي probabilistic model ممكن يعمل clustering للداتا أصلاً؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Regularizationمن الـ Biasلماذا نستثني الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%85%D8%A7%D8%B0%D8%A7-%D9%86%D8%B3%D8%AA%D8%AB%D9%86%D9%8A-%D8%A7%D9%84%D9%80bias-%D9%85%D9%86-%D8%A7%D9%84%D9%80regularization/</link>
    <pubDate>Tue, 30 Nov 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%85%D8%A7%D8%B0%D8%A7-%D9%86%D8%B3%D8%AA%D8%AB%D9%86%D9%8A-%D8%A7%D9%84%D9%80bias-%D9%85%D9%86-%D8%A7%D9%84%D9%80regularization/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـregularization هو أحد الحلول الشائعة لمشكلة الـoverfitting، بيتم عن طريق إضافة penalty مباشرة على الـparameters في الـcost function عشان أقلل الـmodel complexity، لكن لو لاحظت هتلاقي إن الـbias مش موجود في الـpenalty دى على الرغم من إنه أحد الـparameters الى الموديل بيتعلمها، ايه السبب؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Imbalanced Datasets كيف نتعامل مع</title>
    <link>https://Ahmad-Zaki.github.io/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/</link>
    <pubDate>Sat, 20 Nov 2021 21:05:36 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        من المشاكل الى قابلتها في الكورسات إن الـdataset أحياناً بتكون خالية من العيوب الى ممكن نلاقيها في أرض الواقع، وفي مجال الـclassification، فيه فرصة كبيرة إننا نتعامل مع imbalanced datasets لإن الأكيد إن مش كل الـclasses بتحدث بنفس النسبة وبالتالي الغير طبيعي هو إن لما أجمع data أشوف كل الـclasses بنفس الrate ، لكن ليه الـimbalance ده ممكن يسبب مشكلة أصلا؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/</link>
    <pubDate>Sat, 13 Nov 2021 11:11:45 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        ReLU هو اختصار Rectified Linear Unit وهي واحدة من أكثر الـactivation functions المستخدمة في الـNeural networks، رغم إن اسمها يحتوى على كلمة linear لكنها non linear function، وده واضح رياضياً وحتى من الرسم بتاعها، لكن ليه بتقدر تنافس الـactivation functions الأخرى المكوّنة من smooth curves رغم إنها حرفياً عبارة عن خطين بس؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Neural networksفي الـ Non-linear activation functions ليه بنحتاج</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/</link>
    <pubDate>Tue, 09 Nov 2021 13:08:51 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الهدف النهائي الى أنا عايزه من الـneural network هو انها تعبر بشكل تقريبي عن العلاقة ما بين $input \hspace{1mm} (X)$  و $target$ او $output \hspace{1mm} (y)$، بمعنى آخر أنا عايزها تـapproximate دالة $f$ حيث $f(x)=y$.
    </h4>
</div>]]></description>
</item></channel>
</rss>
