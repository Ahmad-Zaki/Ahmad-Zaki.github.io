<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Machine Learning - Tag - Something I learned</title>
        <link>https://Ahmad-Zaki.github.io/tags/machine-learning/</link>
        <description>Machine Learning - Tag - Something I learned</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>ahmadheshamzaki@gmail.com (Ahmad Zaki)</managingEditor>
            <webMaster>ahmadheshamzaki@gmail.com (Ahmad Zaki)</webMaster><lastBuildDate>Tue, 22 Mar 2022 22:00:33 &#43;0200</lastBuildDate><atom:link href="https://Ahmad-Zaki.github.io/tags/machine-learning/" rel="self" type="application/rss+xml" /><item>
    <title>؟Naive Bayes Classifier كيف يعمل</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%8A%D8%B9%D9%85%D9%84-naive-bayes-classifier/</link>
    <pubDate>Tue, 22 Mar 2022 22:00:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%8A%D8%B9%D9%85%D9%84-naive-bayes-classifier/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        Naive Bayes Classifier هو أحد أنواع الـprobabilistic classifiers ومبني على تطبيق بسيط ومباشر لأحد أهم نظريات الاحتمالات: Bayes theorem. ورغم إنه يضع افتراض بسيط وفي الغالب غير مناسب للـdata لكنه سريع ومرن وبيقدم نتائج جيدة ويستخدم في الكثير من التطبيقات كـbaseline.
    </h4>
</div>]]></description>
</item><item>
    <title>؟Vanishing gradientsكيف نتعامل مع مشكلة الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9-%D9%85%D8%B4%D9%83%D9%84%D8%A9-%D8%A7%D9%84%D9%80vanishing-gradients/</link>
    <pubDate>Tue, 14 Dec 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9-%D9%85%D8%B4%D9%83%D9%84%D8%A9-%D8%A7%D9%84%D9%80vanishing-gradients/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـVanishing Gradients هي أحد اكبر المشاكل الى أثرت على تطوّر الـneural networks لفترة طويلة، وبسببها كان تدريب الـdeep neural networks عملية صعبة وبتستغرق وقت كبير، لكن إيه السبب وراء المشكلة دى؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟K-meansمشاكل الـ Gaussian Mixture Modelsكيف تحل الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D8%AA%D8%AD%D9%84-%D8%A7%D9%84%D9%80gaussian-mixture-models-%D9%85%D8%B4%D8%A7%D9%83%D9%84-%D8%A7%D9%84%D9%80k-means/</link>
    <pubDate>Tue, 07 Dec 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D8%AA%D8%AD%D9%84-%D8%A7%D9%84%D9%80gaussian-mixture-models-%D9%85%D8%B4%D8%A7%D9%83%D9%84-%D8%A7%D9%84%D9%80k-means/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـGaussian Mixture models (GMM) هي عبارة عن probabilistic model بيقدر يعبر عن الـdata distribution وكأنه عبارة عن مجموعة gaussian models متجمعة مع بعض، وممكن نستخدمها كـclustering algorithm مشابه للـk-means . بس إزاي probabilistic model ممكن يعمل clustering للداتا أصلاً؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Regularizationمن الـ Biasلماذا نستثني الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%85%D8%A7%D8%B0%D8%A7-%D9%86%D8%B3%D8%AA%D8%AB%D9%86%D9%8A-%D8%A7%D9%84%D9%80bias-%D9%85%D9%86-%D8%A7%D9%84%D9%80regularization/</link>
    <pubDate>Tue, 30 Nov 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%85%D8%A7%D8%B0%D8%A7-%D9%86%D8%B3%D8%AA%D8%AB%D9%86%D9%8A-%D8%A7%D9%84%D9%80bias-%D9%85%D9%86-%D8%A7%D9%84%D9%80regularization/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـregularization هو أحد الحلول الشائعة لمشكلة الـoverfitting، بيتم عن طريق إضافة penalty مباشرة على الـparameters في الـcost function عشان أقلل الـmodel complexity، لكن لو لاحظت هتلاقي إن الـbias مش موجود في الـpenalty دى على الرغم من إنه أحد الـparameters الى الموديل بيتعلمها، ايه السبب؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Imbalanced Datasets كيف نتعامل مع</title>
    <link>https://Ahmad-Zaki.github.io/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/</link>
    <pubDate>Sat, 20 Nov 2021 21:05:36 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        من المشاكل الى قابلتها في الكورسات إن الـdataset أحياناً بتكون خالية من العيوب الى ممكن نلاقيها في أرض الواقع، وفي مجال الـclassification، فيه فرصة كبيرة إننا نتعامل مع imbalanced datasets لإن الأكيد إن مش كل الـclasses بتحدث بنفس النسبة وبالتالي الغير طبيعي هو إن لما أجمع data أشوف كل الـclasses بنفس الrate ، لكن ليه الـimbalance ده ممكن يسبب مشكلة أصلا؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Neural networksفي الـ Non-linear activation functions ليه بنحتاج</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/</link>
    <pubDate>Tue, 09 Nov 2021 13:08:51 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الهدف النهائي الى أنا عايزه من الـneural network هو انها تعبر بشكل تقريبي عن العلاقة ما بين $input \hspace{1mm} (X)$  و $target$ او $output \hspace{1mm} (y)$، بمعنى آخر أنا عايزها تـapproximate دالة $f$ حيث $f(x)=y$.
    </h4>
</div>]]></description>
</item></channel>
</rss>
