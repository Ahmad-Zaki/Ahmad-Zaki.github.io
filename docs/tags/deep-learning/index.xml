<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - Tag - Something I learned</title>
        <link>https://Ahmad-Zaki.github.io/tags/deep-learning/</link>
        <description>Deep Learning - Tag - Something I learned</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>ahmadheshamzaki@gmail.com (Ahmad Zaki)</managingEditor>
            <webMaster>ahmadheshamzaki@gmail.com (Ahmad Zaki)</webMaster><lastBuildDate>Mon, 04 Apr 2022 03:12:33 &#43;0200</lastBuildDate><atom:link href="https://Ahmad-Zaki.github.io/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Image Super-Resolution</title>
    <link>https://Ahmad-Zaki.github.io/image-super-resolution/</link>
    <pubDate>Mon, 04 Apr 2022 03:12:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/image-super-resolution/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        Image Super-resolution (SR) هو الإسم المتداول لعملية تحويل الصورة من Low resolution إلى high resolution عن طريق زيادة عدد الـpixels في الصورة بحيث تحافظ على التفاصيل الموجودة فيها، وهي من أكثر التقنيات أهمية في مجال الـImage processing والـcomputer vision وتدخل في العديد من التطبيقات في الواقع زي تحسين الصور الطبية (Medical Imaging) وصور الأقمار الصناعية وكاميرات المراقبة الـlive-streaming وغيرها الكثير.
    </h4>
</div>]]></description>
</item><item>
    <title>؟ConvNetsفي الـ Backpropagationكيف يتم تنفيذ الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%8A%D8%AA%D9%85-%D8%A7%D9%84%D9%80backpropagation-%D9%81%D9%8A-%D8%A7%D9%84%D9%80covolutional-neural-networks/</link>
    <pubDate>Mon, 28 Feb 2022 14:20:12 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%8A%D8%AA%D9%85-%D8%A7%D9%84%D9%80backpropagation-%D9%81%D9%8A-%D8%A7%D9%84%D9%80covolutional-neural-networks/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        في الغالب كانت بداية معرفتك بالـbackbropagation عن طريق شرحه على Fully connected neural network, لكن بعد ما تتطرق لأشكال أخرى من الـneural netowrks مثل الـconvolutional neural network بتلاقي إن الـbackpropagation مستخدم فيهم كـalgorithm رئيسي لحساب الـgradients في كل layer لكن بدون ذكر تفاصيل أكثر عنها، لإنك بالفعل غير مضطر لمعرفة تفاصيل الرياضية قبل استخدامها، ومع ذلك فمعرفة كيف تتم داخل الـConv layers  قد يكون سؤال مر ببالك ولو من باب الفضول في وقت من الأوقات.
    </h4>
</div>]]></description>
</item><item>
    <title>؟Vanishing gradientsكيف نتعامل مع مشكلة الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9-%D9%85%D8%B4%D9%83%D9%84%D8%A9-%D8%A7%D9%84%D9%80vanishing-gradients/</link>
    <pubDate>Tue, 14 Dec 2021 16:46:33 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9-%D9%85%D8%B4%D9%83%D9%84%D8%A9-%D8%A7%D9%84%D9%80vanishing-gradients/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الـVanishing Gradients هي أحد اكبر المشاكل الى أثرت على تطوّر الـneural networks لفترة طويلة، وبسببها كان تدريب الـdeep neural networks عملية صعبة وبتستغرق وقت كبير، لكن إيه السبب وراء المشكلة دى؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/</link>
    <pubDate>Sat, 13 Nov 2021 11:11:45 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        ReLU هو اختصار Rectified Linear Unit وهي واحدة من أكثر الـactivation functions المستخدمة في الـNeural networks، رغم إن اسمها يحتوى على كلمة linear لكنها non linear function، وده واضح رياضياً وحتى من الرسم بتاعها، لكن ليه بتقدر تنافس الـactivation functions الأخرى المكوّنة من smooth curves رغم إنها حرفياً عبارة عن خطين بس؟
    </h4>
</div>]]></description>
</item><item>
    <title>؟Neural networksفي الـ Non-linear activation functions ليه بنحتاج</title>
    <link>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/</link>
    <pubDate>Tue, 09 Nov 2021 13:08:51 &#43;0200</pubDate>
    <author>Author</author>
    <guid>https://Ahmad-Zaki.github.io/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/</guid>
    <description><![CDATA[<div dir="rtl" style="text-align: justify">
    <h4>
        الهدف النهائي الى أنا عايزه من الـneural network هو انها تعبر بشكل تقريبي عن العلاقة ما بين $input \hspace{1mm} (X)$  و $target$ او $output \hspace{1mm} (y)$، بمعنى آخر أنا عايزها تـapproximate دالة $f$ حيث $f(x)=y$.
    </h4>
</div>]]></description>
</item></channel>
</rss>
