[{"categories":["مما تعلمت"],"content":" من المشاكل الى قابلتها في الكورسات إن الـdataset أحياناً بتكون خالية من العيوب الى ممكن نلاقيها في أرض الواقع، وفي مجال الـclassification، فيه فرصة كبيرة إننا نتعامل مع imbalanced datasets لإن الأكيد إن مش كل الـclasses بتحدث بنفس النسبة وبالتالي الغير طبيعي هو إن لما أجمع data أشوف كل الـclasses بنفس الrate ، لكن ليه الـimbalance ده ممكن يسبب مشكلة أصلا؟ -- Fig 1: Example of an imbalanced dataset ممكن نطرح المشكلة بمثال: لنفترض إن عندي dataset فيها نتائج فحص عن ورم معين في الجسم، فيها 10 ألاف ريكورد عن بيانات مرضى مختلفين والمطلوب مني إن أنا ابني model يتنبأ بوجود الورم ده بناء على بيانات المريض، فأنا دربت موديل بسيط وصلت دقته 99%. نتيجة جيدة بالنسبة لأول تجربة صح؟ في البداية ممكن تتفق، لكن خلينا نلقي نظرة على الdata الأول: لما بصيت على الداتا لقيت إن فيه 100 ريكورد فقط من بين الـ10 آلاف عندهم الورم، والباقي نتايجهم سلبية ، بمعنى آخر: 99% من المرضى الموجودين في الداتا مفيش عندهم ورم. تخيل معايا بقى إن أنا عملت موديل بيفترض إن كل المرضى مفيش عندهم ورم، يعني بيتنبأ إن كل العينات سلبية، الموديل الساذج ده قدر يوصل لـ99% accuracy من غير ما يعمل حاجة! يبقى هنا الـaccuracy أصبحت معيار غير صحيح لنتايج الموديل بتاعي. ليه مقدرش أعتمد على الـaccruacy في مشكلة زى دى؟ $$ Accuracy = \\frac{TP + TN}{TP + FP + TN + FN} $$ $$ TP: True Postive \\hspace{1mm} | \\hspace{1mm} FP: False Positive \\hspace{1mm} | \\hspace{1mm} TN: True Negative \\hspace{1mm} | \\hspace{1mm} FN: False Negative $$ لإن الـaccuracy بتهتم بحساب نسبة الـTrue positive والـTrue negative، في الحالة الى عندي هنا الـNegative يمثّل 99% من الـdata، وبالتالي حتى لما ضحيت بجزء الـtrue positive لم أفقد أي جزء يذكر من الـaccuracy، فكان ده السبب إن الـaccuracy كانت عالية جداً بالنسبة لـmodel حرفياً متعلمش أي حاجة. طب إيه الحل؟ إزاي أقدر احكم على مدى صحة توقعات الموديل في الحالة دى؟ في المشكلة الى عندي، تكلفة إن المريض يبقى عنده ورم وأنا اتنبأ إن معندوش كبيرة جداً: الورم ممكن يكلفه حياته، فبالتالي لازم أختار measure بيتأثر تحديداً بنسبة الـFalse negative وهو الـrecall، لو حسبت الـrecall للموديل البسيط الى عندي هيساوي 0% وهنا تكون النتيجة منطقية. $$ Recall = \\frac{TP}{TP + FN} $$ في حالة أخرى ممكن تكون تكلفة الـFalse positive كبيرة وعندها لازم أختار measure بيتأثر بيه وهو الـPrecision: $$ Precision = \\frac{TP}{TP + FP} $$ ولو محتاج أتابع الأتنين مع بعض يبقى أختار الـF1 score وهو أفضل معيار ممكن استخدمه في حالة التعامل مع Imbalanced data، لإنه بيتأثر بنسبة الـFalse positive والـFalse negative في نتائج الموديل. $$ F_{1} = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$ بعد ما عرفت الـmeasure الى أقدر احكم بيه على نتايج الموديل، الخطوة التالية هي إننا نعمل موديل بيتعلم بكفاءة من الـimbalanced data وبيأدي بشكل جيد سواء على الـmajority class (الى بتمثّل غالبيّة الـdataset) أو الـminority class، طب ليه الـimbalance ممكن يسببلي مشكلة هنا؟ في مرحلة الtraining، الموديل بيهدف إنه يوصل للـparameters الى الـcost عندها أقل ما يمكن، خلال المرحلة دى (ولإن الـmajority class تمثّل غالبية الصفوف في الداتا)، هتلاقي إن معظم الـupdates رايحة ناحية تحسين نتائج الموديل على الـmajority class ، لإن فرصة الـmissclassification فيها أعلى بكتير، وده معناه إنها بتساهم في الـcost بشكل اكبر بكتير من الـminority class، فبالتالي هينتج موديل عنده bias ناحية الـmajority class وبيأدي فيها بشكل أفضل بكتير من الـminority class. Fig 2: مثال على نتائج model بعد تدريبه على imbalanced dataset، لاحظ نسبة الخطأ الكبيرة في الـminority class Source: Click here أحد الطرق الى ممكن تحل مشكلة الـclass imbalance هي إننا نخلق balance بنفسنا عن طريق الـdata resampling وده ممكن يتم بأكتر من طريقة: الطريقة الأولى هي الـRandom Under-sampling: عندك class مسيطر عالداتا ؟ بسيطة: احذف صفوف منه عشوائياً لحد ما يتساوى بالـclass التاني. حل بسيط لكنه غير منطقي، الناس بتحاول تجمع أكبر قدر ممكن من الداتا، وانت رايح تحذفها؟! وهو فعلاً مش أفضل حل لإن حتى لو حلّيت مشكلة الـimbalance هتدخل في مشكلة تانية لإنك فقدت جزء كبير من الداتا ،وكمان ممكن الـsample الصغيرة الى حافظت عليها تكون biased ومش بتعبر عن الـmajority بشكل كويس ،وبالتالي مش هتلاقي نتيجة جيدة. Fig 3: Random Undersampling Vs. Random Oversampling الطريقة التانية هي الـRandom Over-sampli","date":"2021-11-20","objectID":"/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/:0:0","tags":["Machine Learning","Data Science","Data Resampling","SMOTE","Cost-Sensitive Training","Data Analysis","Accurace","Precision","Recall","F1 Score"],"title":"؟Imbalanced Datasets كيف نتعامل مع","uri":"/imbalanced-datasets-%D9%83%D9%8A%D9%81-%D9%86%D8%AA%D8%B9%D8%A7%D9%85%D9%84-%D9%85%D8%B9/"},{"categories":["مما تعلمت"],"content":" ReLU هو اختصار Rectified Linear Unit وهي واحدة من أكثر الـactivation functions المستخدمة في الـNeural networks، رغم إن اسمها يحتوى على كلمة linear لكنها non linear function، وده واضح رياضياً وحتى من الرسم بتاعها، لكن ليه بتقدر تنافس الـactivation functions الأخرى المكوّنة من smooth curves رغم إنها حرفياً عبارة عن خطين بس؟ -- $$ ReLU=max(0,x) $$ Fig 1: ReLU Graph ميزة الـReLU الكبرى مش بتظهر لما أستخدم واحدة بس منها، لكن بتكون واضحة لما أجمع شوية دوال ReLU كتير مع بعض، نقطة الانكسار الموجودة بين جزئين دالة الـReLU بتقدر تقرّبلك مجموعة الخطوط الكتيرة دى من أي منحنى أنت عايزه عن طريق إنها بتتيح لك تكسر الخط المستقيم في أي مكان انت عايزه (أو المكان الى الـneural network بتتعلمه). لاحظ الصورة التالية: لما استخدمت ReLU واحدة بس مقدمتش اختلاف كبير عن الـstraight line، لكن لما بدأت اعمل nesting واجمعهم مع بعض بدأت أكوّن شكل جديد مقدرتش أوصله لما جمعت الـstraight lines مع بعض. Fig 2: ReLU vs. Linear function طيب هل هي فعلاً تقدر توصل لمنحنيات صعبة زى باقي الـactivation function؟ الإجابة القصيرة: نعم، وده واضح من خلال استخدامها الشائع والواسع في كل مجالات الـdeep learning، لاحظ الصورة التالية: عن طريق استخدام أكتر من ReLU function قدرت أوصل لشكل تقريبي من منحنى $x^3+x^2-x-1$ Fig 3: We can use many ReLUs to approximate complex functions Source: Click here وبالنسبة للـNeural networks، في الصورة التالية نقدر نلاحظ إن neural network بتستخدم ReLU كـActivation function وصلت لمنحنى قريب من المنحنى التانج عن neural network بتستخد $tanh$ في نفس عدد الـepoch Fig 4: كل network تحتوي 3 hidden layers، كل منهم يحتوى 3 neurons Source: Click here طيب ما الـtanh وصلت لـsmooth curve وعملت نتيجة كويسة بردو، ليه اروح للـReLU؟ هنا تيجي ميزة الـReLU التانية والى بتديلها أفضلية كبيرة عن الـtanh: إنها very computationally efficient، هي في الآخر عبارة عن خط وتفاضلها ثابت، على عكس الـtanh الى فيها exponentials وتفاضلها متغير. لمزيد من المعلومات Finally, an intuitive explanation of why ReLU works Can neural networks solve any problem? ","date":"2021-11-13","objectID":"/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/:0:0","tags":["Neural Networks","Deep Learning"],"title":"؟Activation functionsالخيار الشائع للـ ReLUليه أصبحت الـ","uri":"/%D9%84%D9%8A%D9%87-%D8%A3%D8%B5%D8%A8%D8%AD%D8%AA-%D8%A7%D9%84%D9%80relu-%D8%A7%D9%84%D8%AE%D9%8A%D8%A7%D8%B1-%D8%A7%D9%84%D8%B4%D8%A7%D8%A6%D8%B9-%D9%84%D9%84%D9%80activation-function/"},{"categories":["مما تعلمت"],"content":" الهدف النهائي الى أنا عايزه من الـneural network هو انها تعبر بشكل تقريبي عن العلاقة ما بين $input \\hspace{1mm} (X)$ و $target$ او $output \\hspace{1mm} (y)$، بمعنى آخر أنا عايزها تـapproximate دالة $f$ حيث $f(x)=y$. هل الـneural networks تقدر تحقق العلاقة دى؟ نظرية الـuniversal approximation بتقول إن اي neural network فيها hidden layer واحدة فقط فيها عدد معيّن من الـneurons، تقدر تعبر عن أي continuous function انت عايزها ولكن لازم تاخد في الاعتبار بعض الشروط، أحد الشروط دى هى الـnon linear activation function. طيب ليه مش هقدر أحقق الـuniversal approximation من غير الـnon linear activation function؟ لإن من غيرها فكل الحسابات الى بتحصل داخل الـneural network ناتجة عن عمليات linear، كلها عمليات ضرب وجمع، لذلك المحصلة النهائية من كل ده هي إن الـneural network مش هتقدر تعبر عن أي حاجة اكتر تعقيداً من linear relationship. طيب هل لو زودت أكتر من hidden layer واحدة هقدر أوصل لـnon linear relationship؟ إضافة hidden layer تانية كتير لا يعني إن انا بضيف non linearity في العلاقة، في الحقيقة كل الـhidden layers دى بتعمل linear transormations متتابعة مش أكتر، والمفاجأة إن كل الـhidden layers الى أنا ضفتها ممكن أستعوض عنهم بـhidden layer واحدة بس تؤدي كل الـlinear transformations دى مرة واحدة، والمفاجأة التانية هى إن كل الـneurons الموجودة في الـhidden layer الواحدة دى ممكن استعوض عنهم بـneuron واحد بس. تفتكر كان إيه اسم الـmodel المكوّن من neuron واحد فقط ومفيش فيه non linear activation function؟ أيوة هو الـlinear regression. فبالتالي أي neural network لا تستخدم activation function قدرتها لا تتعدى قدرة اي linear regression model بسيط. لمزيد من المعلومات: You Don’t Understand Neural Networks Until You Understand the Universal Approximation Theorem Neural Networks and the Universal Approximation Theorem Everything you need to know about “Activation Functions” in Deep learning models ","date":"2021-11-09","objectID":"/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/:0:0","tags":["Machine Learning","Deep Learning","Neural Networks"],"title":"؟Neural networksفي الـ Non-linear activation functions ليه بنحتاج","uri":"/%D9%84%D9%8A%D9%87-%D8%A8%D9%86%D8%AD%D8%AA%D8%A7%D8%AC-non-linear-activation-functions-%D9%81%D9%8A-%D8%A7%D9%84%D9%80neural-networks/"}]